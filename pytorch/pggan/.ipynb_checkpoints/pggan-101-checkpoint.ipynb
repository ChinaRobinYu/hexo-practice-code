{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T05:25:23.023278Z",
     "start_time": "2019-01-24T05:25:22.066990Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rcParams['animation.embed_limit'] = 256\n",
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim, autograd\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "import torch.utils.data as udata\n",
    "import torchvision.datasets as vdatasets\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style(theme='onedork')\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T05:25:23.042445Z",
     "start_time": "2019-01-24T05:25:23.027796Z"
    }
   },
   "outputs": [],
   "source": [
    "class EqualLR:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    def compute_weight(self, module):\n",
    "        weight = getattr(module, self.name + '_orig')\n",
    "        fan_in = weight.data.size(1) * weight.data[0][0].numel()\n",
    "        return weight * np.sqrt(2 / fan_in)\n",
    "\n",
    "    @staticmethod\n",
    "    def apply(module, name):\n",
    "        fn = EqualLR(name)\n",
    "        weight = getattr(module, name)\n",
    "        del module._parameters[name]\n",
    "        module.register_parameter(name + '_orig', nn.Parameter(weight.data))\n",
    "        module.register_forward_pre_hook(fn)\n",
    "        return fn\n",
    "    def __call__(self, module, input):\n",
    "        weight = self.compute_weight(module)\n",
    "        setattr(module, self.name, weight)\n",
    "\n",
    "\n",
    "def equal_lr(module, name='weight'):\n",
    "    EqualLR.apply(module, name)\n",
    "    return module\n",
    "\n",
    "\n",
    "class EqualConv2d(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        conv = nn.Conv2d(*args, **kwargs)\n",
    "        conv.weight.data.normal_()\n",
    "        conv.bias.data.zero_()\n",
    "        self.conv = equal_lr(conv)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.conv(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T05:25:23.134760Z",
     "start_time": "2019-01-24T05:25:23.046514Z"
    }
   },
   "outputs": [],
   "source": [
    "class PixelNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, input):\n",
    "        return input / torch.sqrt(torch.mean(input ** 2, dim=1, keepdim=True) + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T05:25:23.202143Z",
     "start_time": "2019-01-24T05:25:23.137637Z"
    }
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, kernel1, pad1, kernel2, pad2, pixel_norm=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.kernel1 = kernel1\n",
    "        self.kernel2 = kernel2\n",
    "        self.stride1 = 1\n",
    "        self.stride2 = 1\n",
    "        self.pad1 = pad1\n",
    "        self.pad2 = pad2\n",
    "        \n",
    "        if pixel_norm:\n",
    "            self.conv = nn.Sequential(EqualConv2d(in_channel, out_channel, self.kernel1, self.stride1, self.pad1),\n",
    "                                      PixelNorm(),\n",
    "                                      nn.LeakyReLU(0.2),\n",
    "                                      EqualConv2d(out_channel, out_channel, self.kernel2, self.stride2, self.pad2),\n",
    "                                      PixelNorm(),\n",
    "                                      nn.LeakyReLU(0.2))\n",
    "        else:\n",
    "            self.conv = nn.Sequential(EqualConv2d(in_channel, out_channel, self.kernel1, self.stride1, self.pad1),\n",
    "                                      nn.LeakyReLU(0.2),\n",
    "                                      EqualConv2d(out_channel, out_channel, self.kernel2, self.stride2, self.pad2),\n",
    "                                      nn.LeakyReLU(0.2))\n",
    "    def forward(self, input):\n",
    "        out = self.conv(input)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T05:25:23.328380Z",
     "start_time": "2019-01-24T05:25:23.205008Z"
    }
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, code_dim=512):\n",
    "        super().__init__()\n",
    "        self.code_norm = PixelNorm()\n",
    "        self.progression = nn.ModuleList([ConvBlock(512, 512, 4, 3, 3, 1),\n",
    "                                          ConvBlock(512, 512, 3, 1, 3, 1),\n",
    "                                          ConvBlock(512, 512, 3, 1, 3, 1),\n",
    "                                          ConvBlock(512, 512, 3, 1, 3, 1),\n",
    "                                          ConvBlock(512, 256, 3, 1, 3, 1),\n",
    "                                          ConvBlock(256, 128, 3, 1, 3, 1)])\n",
    "        self.to_rgb = nn.ModuleList([nn.Conv2d(512, 3, 1),\n",
    "                                     nn.Conv2d(512, 3, 1),\n",
    "                                     nn.Conv2d(512, 3, 1),\n",
    "                                     nn.Conv2d(512, 3, 1),\n",
    "                                     nn.Conv2d(256, 3, 1),\n",
    "                                     nn.Conv2d(128, 3, 1),])\n",
    "        \n",
    "    def forward(self, input, expand=0, alpha=-1):\n",
    "        out = self.code_norm(input)\n",
    "        for i, (conv, to_rgb) in enumerate(zip(self.progression, self.to_rgb)):\n",
    "            if i > 0 and expand > 0:\n",
    "                upsample = F.interpolate(out, scale_factor=2)\n",
    "                out = conv(upsample)\n",
    "            else:\n",
    "                out = conv(out)\n",
    "                \n",
    "            if i == expand:\n",
    "                out = to_rgb(out)\n",
    "                \n",
    "                if i > 0 and 0 <= alpha < 1:\n",
    "                    skip_rgb = self.to_rgb[i - 1](upsample)\n",
    "                    out = (1 - alpha) * skip_rgb + alpha * out\n",
    "                break\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T05:25:23.420730Z",
     "start_time": "2019-01-24T05:25:23.331446Z"
    }
   },
   "outputs": [],
   "source": [
    "class Distriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.progression = nn.ModuleList([ConvBlock(128, 256, 3, 1, 3, 1, pixel_norm=False),\n",
    "                                          ConvBlock(256, 512, 3, 1, 3, 1, pixel_norm=False),\n",
    "                                          ConvBlock(512, 512, 3, 1, 3, 1, pixel_norm=False),\n",
    "                                          ConvBlock(512, 512, 3, 1, 3, 1, pixel_norm=False),\n",
    "                                          ConvBlock(512, 512, 3, 1, 3, 1, pixel_norm=False),\n",
    "                                          ConvBlock(513, 512, 3, 1, 4, 0, pixel_norm=False),])\n",
    "        self.from_rgb = nn.ModuleList([nn.Conv2d(3, 128, 1),\n",
    "                                       nn.Conv2d(3, 256, 1),\n",
    "                                       nn.Conv2d(3, 512, 1),\n",
    "                                       nn.Conv2d(3, 512, 1),\n",
    "                                       nn.Conv2d(3, 512, 1),\n",
    "                                       nn.Conv2d(3, 512, 1),])\n",
    "        self.n_layer = len(self.progression)\n",
    "        self.linear = nn.Linear(512, 1)\n",
    "    \n",
    "    def forward(self, input, expand=0, alpha=-1):\n",
    "        for i in range(expand, -1, -1):\n",
    "            index = self.n_layer - i - 1\n",
    "            if i == expand:\n",
    "                out = self.from_rgb[index](input)\n",
    "            if i == 0:\n",
    "                mean_std = input.std(0).mean()\n",
    "                mean_std = mean_std.expand(input.size(0), 1, 4, 4)\n",
    "                out = torch.cat([out, mean_std], 1)\n",
    "            out = self.progression[index](out)\n",
    "            \n",
    "            if i > 0:\n",
    "                out = F.avg_pool2d(out, 2)\n",
    "                if i == expand and 0 <= alpha < 1:\n",
    "                    skip_rgb = F.avg_pool2d(input, 2)\n",
    "                    skip_rgb = self.from_rgb[index + 1](skip_rgb)\n",
    "                    out = (1 - alpha) * skip_rgb + alpha * out\n",
    "                    \n",
    "        out = out.squeeze(2).squeeze(2)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T05:25:23.511188Z",
     "start_time": "2019-01-24T05:25:23.423771Z"
    }
   },
   "outputs": [],
   "source": [
    "dataroot = '/home/samael/github/image_generation/dcgan/'\n",
    "workers = 8\n",
    "# batch size map {4:32, 8:32, 16:32, 32:16, 64:16, 128:16, 256:12, 512:3, 1024:1}\n",
    "batch_size = 32\n",
    "image_size = 4\n",
    "nc = 3\n",
    "nz = 512\n",
    "num_epochs = 300\n",
    "ngpu = 2\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T05:25:34.195342Z",
     "start_time": "2019-01-24T05:25:23.514226Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc7c8a41940>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def modify_data(root, image_size):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.CenterCrop(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    dataset = vdatasets.ImageFolder(root=root, transform=transform)\n",
    "    return dataset\n",
    "\n",
    "dataset = modify_data(dataroot, image_size)\n",
    "\n",
    "dataloader = udata.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
    "real_batch = next(iter(dataloader))\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:16], padding=1, normalize=True, nrow=4).cpu(),(1,2,0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T05:25:35.868332Z",
     "start_time": "2019-01-24T05:25:34.202883Z"
    }
   },
   "outputs": [],
   "source": [
    "netG = Generator(nz).to(device)\n",
    "netD = Distriminator().to(device)\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netG = nn.DataParallel(netG, list(range(ngpu)))\n",
    "    netD = nn.DataParallel(netD, list(range(ngpu)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T05:25:35.882254Z",
     "start_time": "2019-01-24T05:25:35.873309Z"
    }
   },
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "lr = 0.001\n",
    "beta1 = 0.0\n",
    "g_optimizer = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.99))\n",
    "d_optimizer = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T06:22:27.074477Z",
     "start_time": "2019-01-24T05:25:35.885647Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start!\n",
      "[1/300][200/2188](200)\tLoss_D: 0.1856\tLoss_G: 0.4442\tD(x): -0.1759\tD(G(z)): -0.3616\tGrad: 0.0577\n",
      "[1/300][400/2188](400)\tLoss_D: 1.3271\tLoss_G: 2.6819\tD(x): -0.4560\tD(G(z)): -1.7831\tGrad: 0.2399\n",
      "[1/300][600/2188](600)\tLoss_D: 0.2456\tLoss_G: 0.3145\tD(x): 0.1867\tD(G(z)): -0.0589\tGrad: 0.0624\n",
      "[1/300][800/2188](800)\tLoss_D: 0.3652\tLoss_G: 0.1997\tD(x): -0.0211\tD(G(z)): -0.3864\tGrad: 0.0897\n",
      "[1/300][1000/2188](1000)\tLoss_D: 0.2306\tLoss_G: 0.5143\tD(x): 0.0869\tD(G(z)): -0.1437\tGrad: 0.0219\n",
      "[1/300][1200/2188](1200)\tLoss_D: 0.1224\tLoss_G: 0.4189\tD(x): -0.1651\tD(G(z)): -0.2874\tGrad: 0.1801\n",
      "[1/300][1400/2188](1400)\tLoss_D: 0.2481\tLoss_G: 0.1582\tD(x): 0.0419\tD(G(z)): -0.2063\tGrad: 0.0261\n",
      "[1/300][1600/2188](1600)\tLoss_D: 0.3606\tLoss_G: 0.4014\tD(x): -0.0774\tD(G(z)): -0.4381\tGrad: 0.0909\n",
      "[1/300][1800/2188](1800)\tLoss_D: 0.1140\tLoss_G: 0.2680\tD(x): -0.1480\tD(G(z)): -0.2619\tGrad: 0.1155\n",
      "[1/300][2000/2188](2000)\tLoss_D: 0.3516\tLoss_G: 0.2222\tD(x): 0.1549\tD(G(z)): -0.1967\tGrad: 0.1631\n",
      "[2/300][12/2188](2200)\tLoss_D: -0.1506\tLoss_G: -0.0775\tD(x): 0.0636\tD(G(z)): 0.2142\tGrad: 0.0474\n",
      "[2/300][212/2188](2400)\tLoss_D: 0.1313\tLoss_G: 0.3864\tD(x): 0.0651\tD(G(z)): -0.0663\tGrad: 0.0576\n",
      "[2/300][412/2188](2600)\tLoss_D: 0.3861\tLoss_G: 0.4367\tD(x): -0.0772\tD(G(z)): -0.4633\tGrad: 0.1073\n",
      "[2/300][612/2188](2800)\tLoss_D: 0.1503\tLoss_G: 0.2865\tD(x): -0.0568\tD(G(z)): -0.2071\tGrad: 0.1034\n",
      "[2/300][812/2188](3000)\tLoss_D: 0.1210\tLoss_G: 0.1901\tD(x): -0.0866\tD(G(z)): -0.2076\tGrad: 0.0845\n",
      "[2/300][1012/2188](3200)\tLoss_D: 0.3337\tLoss_G: 0.3104\tD(x): 0.0097\tD(G(z)): -0.3239\tGrad: 0.2417\n",
      "[2/300][1212/2188](3400)\tLoss_D: 0.3674\tLoss_G: 0.2128\tD(x): 0.0366\tD(G(z)): -0.3309\tGrad: 0.1166\n",
      "[2/300][1412/2188](3600)\tLoss_D: 0.4148\tLoss_G: 0.2754\tD(x): 0.1432\tD(G(z)): -0.2716\tGrad: 0.1088\n",
      "[2/300][1612/2188](3800)\tLoss_D: 0.2601\tLoss_G: 0.4158\tD(x): -0.0782\tD(G(z)): -0.3382\tGrad: 0.1081\n",
      "[2/300][1812/2188](4000)\tLoss_D: 0.2438\tLoss_G: 0.1602\tD(x): 0.0091\tD(G(z)): -0.2347\tGrad: 0.0693\n",
      "[2/300][2012/2188](4200)\tLoss_D: 0.1488\tLoss_G: 0.2215\tD(x): 0.0662\tD(G(z)): -0.0826\tGrad: 0.1109\n",
      "[3/300][24/2188](4400)\tLoss_D: 0.1184\tLoss_G: 0.1241\tD(x): 0.5049\tD(G(z)): 0.3865\tGrad: 0.0407\n",
      "[3/300][224/2188](4600)\tLoss_D: 0.3932\tLoss_G: 0.8672\tD(x): 0.0555\tD(G(z)): -0.3377\tGrad: 0.0617\n",
      "[3/300][424/2188](4800)\tLoss_D: 0.5435\tLoss_G: 1.4142\tD(x): -0.1563\tD(G(z)): -0.6998\tGrad: 0.1796\n",
      "[3/300][624/2188](5000)\tLoss_D: 0.8383\tLoss_G: 0.8856\tD(x): -0.2469\tD(G(z)): -1.0852\tGrad: 0.2163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-31:\n",
      "Process Process-32:\n",
      "Process Process-30:\n",
      "Process Process-28:\n",
      "Process Process-26:\n",
      "Process Process-25:\n",
      "Process Process-27:\n",
      "Process Process-29:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 132, in __getitem__\n",
      "    sample = self.loader(path)\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 132, in __getitem__\n",
      "    sample = self.loader(path)\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 132, in __getitem__\n",
      "    sample = self.loader(path)\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 132, in __getitem__\n",
      "    sample = self.loader(path)\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 161, in pil_loader\n",
      "    return img.convert('RGB')\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 132, in __getitem__\n",
      "    sample = self.loader(path)\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 178, in default_loader\n",
      "    return pil_loader(path)\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 178, in default_loader\n",
      "    return pil_loader(path)\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 132, in __getitem__\n",
      "    sample = self.loader(path)\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 899, in convert\n",
      "    self.load()\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 161, in pil_loader\n",
      "    return img.convert('RGB')\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 132, in __getitem__\n",
      "    sample = self.loader(path)\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 178, in default_loader\n",
      "    return pil_loader(path)\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 908, in convert\n",
      "    return self.copy()\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 178, in default_loader\n",
      "    return pil_loader(path)\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 1092, in copy\n",
      "    return self._new(self.im.copy())\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 178, in default_loader\n",
      "    return pil_loader(path)\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 178, in default_loader\n",
      "    return pil_loader(path)\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 161, in pil_loader\n",
      "    return img.convert('RGB')\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py\", line 221, in load\n",
      "    s = read(self.decodermaxblock)\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 178, in default_loader\n",
      "    return pil_loader(path)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 161, in pil_loader\n",
      "    return img.convert('RGB')\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 132, in __getitem__\n",
      "    sample = self.loader(path)\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 899, in convert\n",
      "    self.load()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py\", line 239, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 899, in convert\n",
      "    self.load()\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 178, in default_loader\n",
      "    return pil_loader(path)\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/PIL/JpegImagePlugin.py\", line 362, in load_read\n",
      "    s = self.fp.read(read_bytes)\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 161, in pil_loader\n",
      "    return img.convert('RGB')\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 161, in pil_loader\n",
      "    return img.convert('RGB')\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 161, in pil_loader\n",
      "    return img.convert('RGB')\n",
      "KeyboardInterrupt\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py\", line 239, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 161, in pil_loader\n",
      "    return img.convert('RGB')\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 899, in convert\n",
      "    self.load()\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 908, in convert\n",
      "    return self.copy()\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py\", line 239, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 899, in convert\n",
      "    self.load()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 899, in convert\n",
      "    self.load()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py\", line 239, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py\", line 221, in load\n",
      "    s = read(self.decodermaxblock)\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 1092, in copy\n",
      "    return self._new(self.im.copy())\n",
      "  File \"/home/samael/anaconda3/lib/python3.6/site-packages/PIL/JpegImagePlugin.py\", line 362, in load_read\n",
      "    s = self.fp.read(read_bytes)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-650fbc66ff95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mx_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mhat_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mgrad_x_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhat_predict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mgrad_penalty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_x_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_x_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mgrad_penalty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad_penalty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    143\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[1;32m    144\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         inputs, allow_unused)\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "experiment_path = 'checkpoint/pggan'\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "D_losses_tmp = []\n",
    "Grad_penalty = []\n",
    "i = 0\n",
    "iters = 0\n",
    "total_iters = 0\n",
    "expand = 0\n",
    "n_critic = 1\n",
    "step = 0\n",
    "alpha = 0\n",
    "CLAMP = 0.01\n",
    "one = torch.FloatTensor([1]).cuda()\n",
    "mone = one * -1\n",
    "print('Training start!')\n",
    "for epoch in range(num_epochs):\n",
    "    if epoch != 0 and epoch % 50 == 0:\n",
    "        alpha = 0\n",
    "        iters = 0\n",
    "        expand += 1\n",
    "        if expand >= 3:\n",
    "            batch_size = 16\n",
    "        if expand > 5:\n",
    "            alpha = 1\n",
    "            expand = 5\n",
    "        dataset = modify_data(dataroot, image_size * 2 ** expand)\n",
    "        dataloader = udata.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
    "    for i, data in enumerate(dataloader):\n",
    "        real_cpu = data[0].to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        if step < n_critic:\n",
    "            netD.zero_grad()\n",
    "            for p in netD.parameters():\n",
    "                p.requires_grad = True\n",
    "#                 p.data.clamp_(-CLAMP, CLAMP)\n",
    "            output = netD(real_cpu, expand, alpha).view(-1)\n",
    "            errD_real = (output.mean() - 0.001 * (output ** 2).mean()).view(1)\n",
    "            errD_real.backward(mone)\n",
    "            noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "            fake = netG(noise, expand, alpha)\n",
    "            output = netD(fake.detach(), expand, alpha).view(-1)\n",
    "            errD_fake = output.mean().view(1)\n",
    "            errD_fake.backward(one)\n",
    "            eps = torch.rand(b_size, 1, 1, 1, device=device)\n",
    "            x_hat = eps * real_cpu.data + (1 - eps) * fake.data\n",
    "            x_hat.requires_grad = True\n",
    "            hat_predict = netD(x_hat, expand, alpha)\n",
    "            grad_x_hat = autograd.grad(outputs=hat_predict.sum(), inputs=x_hat, create_graph=True)[0]\n",
    "            grad_penalty = ((grad_x_hat.view(grad_x_hat.size(0), -1).norm(2, dim=1) - 1) ** 2).mean()\n",
    "            grad_penalty = 10 * grad_penalty\n",
    "            grad_penalty.backward()\n",
    "            errD = errD_real - errD_fake\n",
    "            d_optimizer.step()\n",
    "            D_losses_tmp.append(errD.item())\n",
    "            step += 1\n",
    "        else:\n",
    "            for p in netD.parameters():\n",
    "                p.requires_grad = False\n",
    "            netG.zero_grad()\n",
    "            noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "            fake = netG(noise, expand, alpha)\n",
    "            output = netD(fake, expand, alpha).view(-1)\n",
    "            errG = -output.mean().view(1)\n",
    "            errG.backward()\n",
    "            g_optimizer.step()\n",
    "            D_losses.append(np.mean(D_losses_tmp))\n",
    "            G_losses.append(errG.item())\n",
    "            D_losses_tmp = []\n",
    "            step = 0\n",
    "        if (total_iters+1) % 200 == 0:\n",
    "            print('[%d/%d][%d/%d](%d)\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f\\tGrad: %.4f'\n",
    "                  % (epoch+1, num_epochs, i+1, len(dataloader), total_iters + 1,\n",
    "                     errD.item(), errG.item(), errD_real.data.mean(), errD_fake.data.mean(), grad_penalty.data))\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (total_iters % 5000 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise, expand, alpha).detach().cpu()\n",
    "            img = vutils.make_grid(fake, padding=2, normalize=True)\n",
    "            vutils.save_image(img, 'checkpoint/pggan/fake_image/fake_iter_{0}.jpg'.format(total_iters))\n",
    "            img_list.append(img)\n",
    "\n",
    "        iters += 1\n",
    "        total_iters += 1\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            torch.save(netG.state_dict(), '{0}/netG_epoch_{1}.pth'.format(experiment_path, epoch+1))\n",
    "            torch.save(netD.state_dict(), '{0}/netD_epoch_{1}.pth'.format(experiment_path, epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T06:22:27.093978Z",
     "start_time": "2019-01-24T05:25:22.091Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T06:22:27.096196Z",
     "start_time": "2019-01-24T05:25:22.093Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,15))\n",
    "plt.axis(\"off\")\n",
    "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
